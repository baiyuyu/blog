---
layout: post
title: 超参数调节方法
subtitle:  机器学习
date: 2020-07-01
published: True
mathjax: True
catalog: true
tags:
  - 机器学习

---
# TOC
1. 简介
{:toc}

# 
一般来讲，超参数调节的方法有三种
网格搜索
梯度搜索
贝叶斯优化

*********************
* 知识回顾：
k折交叉验证
一般情况将K折交叉验证用于模型调优，找到使得模型泛化性能最优的超参值。，找到后，在全部训练集上重新训练模型，并使用独立测试集对模型性能做出最终评价。

K折交叉验证使用了无重复抽样技术的好处：每次迭代过程中每个样本点只有一次被划入训练集或测试集的机会。



K折交叉验证图：





如果训练数据集相对较小，则增大k值。

增大k值，在每次迭代过程中将会有更多的数据用于模型训练，能够得到最小偏差，同时算法时间延长。且训练块间高度相似，导致评价结果方差较高。

 

如果训练集相对较大，则减小k值。

减小k值，降低模型在不同的数据块上进行重复拟合的性能评估的计算成本，在平均性能的基础上获得模型的准确评估。

*********************


# 网格搜索
# 随机搜索

# 贝叶斯优化
妈呀，我其实真的挺烦这位贝叶斯先生。到处都是贝叶斯贝叶斯，当然，也烦高斯先生，好吧，不吐槽了，人家就是比较厉害，我理解不了人家的思维而已。。
总而言之，言而总之，还是要学学这两位先生的理论。谁让我是机器学习工程师了。
贝叶斯先生最出名的就是贝叶斯公式，条件概率公式。先验概率就是某件事情发生的概率。然后时间a已经发生的情况下，
更新时间b发生的概率。即条件概率，也就是后验证概率。

贝叶斯优化，采用的就是这种思想。
假设：参数与结果具有某种分布（代理模型）。可以算出一些估计值，在置信度alpha的条件下，这种分布的概率是多少。这个是先验。
然后算一组参数，一组结果。已发生的事件，然后算出后验概率。
每次都用（采集函数）获得下一次要计算的值，使得下一次计算的值可能是最优解。
从而计算出的后验概率更大。

不断迭代，就可以快速确定这个概率最大的分布，就知道最大值了。采用了贝叶斯的思想，每算出一个点，概率就会增加一些。
采集函数：可以是不确定性，每次计算当前不确定性最大的点，最大提升概率（MPI）



                   
# 总结

